## NOTES:

1. **Expected directory structure**
```
project/
â”œâ”€â”€ input_files/
â”‚   â”œâ”€â”€ combined_training.extxyz
â”‚   â””â”€â”€ temp_300.0
â”œâ”€â”€ input.yaml
â”œâ”€â”€ nvt_simulation.in
â”œâ”€â”€ run_allegro.sh
â”œâ”€â”€ training_files/
â””â”€â”€ h2o-deployed.pth
```
- **`project/`** â†’ Example: `H2O`
- **`combined_training.extxyz`** â†’ Contains all DFT frames; training/validation split is managed via `input.yaml`.
- **`temp_300.0/`** â†’ Folder with the initial system snapshot generated by `box_initializer.py`.
- **`nvt_simulation.in`** â†’ LAMMPS input file for NVT (or NPT) simulations.
- **`input.yaml`** â†’ Allegro input configuration file.
- **`run_allegro.sh`** â†’ SLURM `sbatch` script for job execution.
- **`training_files/`** â†’ Stores logs, trained models, and datasets.
- **`h2o-deployed.pth`** â†’ Final trained model.

---

## ğŸ› ï¸ Run Script Best Practices
- **Avoid combining training and simulation run scripts** unless necessary.
- Training can be time-intensive, particularly for complex models.
- The **Allegro MLIP** developers provide configuration recommendations:  
  ğŸ”— [Nequip Full Config](https://github.com/mir-group/nequip/blob/main/configs/full.yaml)

---

## ğŸ–¥ï¸ CPU Usage for Training
- Using **multiple CPU cores** is recommended due to improved **data parsing parallelization** (from version **0.5.5+**).
- For details, see:  
  ğŸ”— [Discussion: mir-group/nequip#182](https://github.com/mir-group/nequip/issues/182)  
  or consult the **0.5.5+ documentation**.

---

## ğŸ“Š Training, Validation & Test Sets
- Ensure that the **sum of training + validation data is < 100%** to leave space for testing.
- To evaluate trained models, use:
  ```bash
  nequip-evaluate

   - project -> e.g H2O
   - combined_training -> extxyz containing all DFT frames, training and validation split is done using
                    the input.yaml options.
   - temp_300.0 folder containing initializing snapshot generated by box_initializer.py
   - nvt_simulation.in -> LAMMPS input settings file for an NVT run, could also be NPT.
   - input.yaml -> Allegro input settings file
   - run_allegro.sh -> Job SBatch script
   - training_files -> contains logs, models, data from training
   - h2o-deployed.pth -> Final model derived from training

2. **Combining Run Scripts**
   - While it is technically possible to combine both run scripts, **it is not recommended**.  
   - Training, especially for complex models, can take a significant amount of time.  
   - Users should assess the **model complexity** based on their desired outputs.  
   - The creators of the Allegro MLIP provide recommendations in their `configs.yaml` file:  
     ğŸ”— [Nequip Full Config](https://github.com/mir-group/nequip/blob/main/configs/full.yaml)

3. **CPU Usage for Training**
   - Using **more CPU cores** is highly recommended for training due to the implementation of data parsing 
     parallelization from **version 0.5.5 onwards**
   - For more details, refer to the following discussion:  
     ğŸ”— [mir-group/nequip#182](https://github.com/mir-group/nequip/issues/182)  
   - Alternatively, review the **0.5.5 documentation** for a deeper understanding of the update.

4. **Managing Training, Validation, and Test Sets**
   - If you plan to create test sets, ensure that in your `input.yaml` file,  
     **the sum of training and validation datasets does not reach 100%**.  
   - You must leave some data aside for **testing**.  
   - Testing is performed using the following command:
     ```bash
     nequip-evaluate
     ```

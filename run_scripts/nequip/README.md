## NOTES:

1. **Expected directory structure**
```
project/
â”œâ”€â”€ input_files/
â”‚   â”œâ”€â”€ combined_training.extxyz
â”‚   â””â”€â”€ temp_300.0
â”œâ”€â”€ input.yaml
â”œâ”€â”€ nvt_simulation.in
â”œâ”€â”€ run_nequip.sh
â”œâ”€â”€ training_files/
â””â”€â”€ h2o-deployed.pth
```
- **`project/`** â†’ Example: `H2O`
- **`combined_training.extxyz`** â†’ Contains all DFT frames; training/validation split is managed via `input.yaml`.
- **`temp_300.0/`** â†’ Folder with the initial system snapshot generated by `box_initializer.py`.
- **`nvt_simulation.in`** â†’ LAMMPS input file for NVT (or NPT) simulations.
- **`input.yaml`** â†’ Allegro input configuration file.
- **`run_nequip.sh`** â†’ SLURM `sbatch` script for job execution.
- **`training_files/`** â†’ Stores logs, trained models, and datasets.
- **`h2o-deployed.pth`** â†’ Final trained model.

---

## ğŸ› ï¸ Run Script Best Practices
- **Avoid combining training and simulation run scripts** unless necessary.
- Training can be time-intensive, particularly for complex models and retraining these models are unneccessary.
- The **NequIP MLIP** developers provide configuration recommendations:  
  ğŸ”— [Nequip Full Config](https://github.com/mir-group/nequip/blob/main/configs/full.yaml)

---

## ğŸ–¥ï¸ CPU Usage for Training
- Using **multiple CPU cores** is recommended due to improved speed from **data parsing parallelization** (from version **0.5.5+**).
- For details, see:  
  ğŸ”— [Discussion: mir-group/nequip#182](https://github.com/mir-group/nequip/issues/182)  
  or consult the **0.5.5+ documentation**.

---

## ğŸ“Š Training, Validation & Test Sets
- Ensure that the **sum of training + validation data is < 100%** to leave datasets for testing.
- To evaluate trained models, use:
  ```bash
  nequip-evaluate
  ```
